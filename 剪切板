4）总体流程（先“像人”地听，再量化评分）

P.563 的思路可以想象为“一个专家拿普通话机在旁边听”：
先建模接收端手柄→语音活动检测（VAD）→电平归一化到 −26 dBov→把预处理后的语音送入多路“传感器”分析，提取一组关键参数；再根据这些关键参数判定主失真类别，最后由“语音质量模型”给出 MOS-LQO 分数。
预处理细节：VAD 采用自适应门限并迭代估计；之后应用IRS 接收特性滤波；并进行**−26 dBov 归一化和100 Hz 高通等处理（不同后续模块使用 IRS 滤波后或仅归一化的信号；静音/中断模块要用原始信号）。
VAD 的门限从全局均值起步，迭代为“噪声段均值 + 2×标准差”，并对极短脉冲（≤12 ms）与过近切分（<200 ms）**做合理合并/标注。


总之，我们的研究针对在从LTE到Wi-Fi网络过渡过程中选择Wi-Fi接入点的关键挑战，旨在改善VoIP通话的服务质量（QoS）。\
我们将基于多层感知器（MLP）和长短期记忆网络（LSTM）的创新MOS预测模型整合到SDN控制器中，并使用灰狼优化（GWO）算法进行了优化。这一整合显著提升了各种QoS参数。\
两个MOS预测模型大幅降低了数据包丢失率，从2%降低到不足1%。值得注意的是，LSTM模型展现了长期稳定性，并在选择用户设备（UE）时相较于MLP模型表现更优。\
MOS预测模型的引入使抖动显著减少，从25毫秒降低到不足7.6毫秒。基于LSTM的模型始终优于MLP模型，改进幅度达15%，当UE选择基于SNIR时，得到了最佳结果。\
两个预测模型在短期内都保持了低于150毫秒的令人满意的延迟。然而，随着时间的推移，MLP模型延迟略微上升，而LSTM模型则稳定维持在140毫秒（RSSI）和90毫秒（SNIR）以下。\
对于这两个预测模型，显著的吞吐量增加被观察到，范围为0.3 Mbps（基于RSSI的MLP）到0.6 Mbps（基于SNIR的LSTM）。\
为了为各类数据密集型应用提供更好的用户体验，这些提升是必不可少的。MOS值始终超过了3.5的卸载阈值，范围在4.2到4.6之间。\
特别是当结合SNIR选择时，LSTM模型展现了优越的性能，达到了4.5的MOS值。

1. MOS prediction by SDN controller and User Equipment to maintain good quality for VoIP over WiFi
目标：解决WiFi中VoIP用户切换时的音质下降问题。
方法：
移动节点端：利用 MLP（多层感知机）根据网络参数（时延、抖动、丢包率、播放丢失）预测 MOS，提前触发切换。
SDN 控制器端：用 KNN 对接入点进行分类，选择最优接入点。
阈值：当预测MOS < 4.03时准备切换，MOS = 3.7时触发切换。
结果：相比基于 RSSI 的切换，VoIP MOS 保持在 4.35 左右，丢包率不超过 1.2%。
2. Deep Learning Driven QoS Anomaly Detection for Network Performance Optimization

3. Improved VoLTE QoE Estimation Procedure using Network Performance Metrics
目标：提升VoLTE用户主观体验（QoE）的估计精度。
方法：
QoS 参数：丢包、网络抖动、抖动缓冲区大小。
传统 G.107 MOS 在网络侧监控得到，但与用户真实感受差距较大。
提出 VM_MOS 方法，基于优化的 QoE/QoS 数学关系：

Ppl_eff 同时考虑丢包和抖动的综合影响。
结果：VM_MOS 与 POLQA（ITU P.863 标准）差距仅 0.02~0.15，明显优于传统 G.107。
意义：能在无需终端测试的情况下，从网络侧监控得到贴近用户感受的MOS估计。

1. A mean opinion score prediction model for VoIP calls offloading handover from LTE to WiFi
目标：解决 LTE ↔ WiFi 切换时 VoIP 通话质量下降的问题。
方法：基于网络参数（时延、丢包、抖动），用 多层感知机（MLP）模型来预测 MOS。
应用场景：在 offloading handover 前预测 MOS，如果 MOS 低于某阈值，则提前触发切换。
结果：比传统 RSSI 触发方式更能维持 MOS 在“良好”水平。
贡献：证明 MLP 神经网络在切换决策中能更准确地反映用户体验。
2. Towards Reduced Reference Parametric Models for Estimating Audiovisual Quality in Multimedia Services
目标：在多媒体（音频+视频）服务中开发 Reduced Reference (RR) 模型 来估计用户感知质量。
方法：
数据库：构建了 144 种不同网络条件（码率、带宽、丢包、抖动）的音视频测试集。
模型：用 Random Forest (RF) 和 Multi-layer Perceptron (MLP) 回归预测 MOS。
结果：RF 优于 MLP（RMSE=0.31 vs 0.42, PCC=0.89 vs 0.80）。
贡献：证明 RF 对参数型 QoE 预测更适合，并强调丢包率、抖动和带宽对感知质量影响最大。
3. Non-Intrusive Parametric Audio Quality Estimation Models for Broadcasting Systems and Web-Casting Applications Based on Random Forest
目标：开发用于 广播和网络音频传输（如 Spotify, DAB, DRM） 的非侵入式音频质量预测模型。
方法：
输入特征：音频 编码格式、比特率、初始延迟、卡顿 (stalling) 等。
数据：17,280 条 Web-cast 样本 + 1,080 条广播样本。
模型：Random Forest 回归预测 MOS。
结果：
Web-cast：PCC=0.985, RMSE≈0.22
Broadcast：PCC=0.941, RMSE≈0.20
实时计算负载低，可嵌入监测系统。
贡献：第一个专注于 广播/网络音频场景的非侵入式参数模型，适合大规模实时监测。
4. An Effective Machine Learning (ML) Approach to Quality Assessment of Voice Over IP (VoIP) Calls
目标：改进传统 VQmon/E-model 在 VoIP 宽带语音质量预测上的不足。
方法：
进行主观听测（ACR 5分制），收集 56 人对 12 种网络场景（时延、丢包）的评分。
引入 Ordinal Logistic Regression (OLR)，并与 决策树 (DT)、随机森林 (RF)、多项逻辑回归 (MLR) 对比。
结果：
OLR 准确率 61%，优于 DT (51%) 和 RF (48%)。
在二分类场景下，LogReg 准确率可达 83%。
贡献：证明 OLR 能更好地捕捉 QoE 的 有序特性（1–5 分），比传统模型更符合用户主观体验。

📌 总结对比：

特征一：CQT（Constant‑Q Transform，常 Q 变换，用于 CNN）
把语音看成“随时间变化的频率能量图”。传统 STFT 用固定窗口，高频和低频的“分辨率”不一样，跟人耳的频带分辨率并不完全匹配。
CQT 的频带是按几何级数划分，让每个频带的品质因数 Q=中心频率/带宽恒定，更贴近人耳在 500 Hz–20 kHz 的“恒定 Q”感知特性。
这篇论文怎么做
先把一段语音转成 CQT 频谱图（二维图像），再输入 CNN。
为了让 CNN 的输入尺寸固定，作者把频谱帧数统一到 220 帧：短语音就复制最后一帧补齐，长语音就裁掉多余帧；最终尺寸定为 240 × 220（频率 × 时间）。
为什么它适合做 MOS 预测？
CQT 在低频有更高分辨率，对共振峰、混响拖尾、低频噪声更敏感；在高频有更宽带宽，能概括高频粗糙度。
这些都与人对“清晰/闷/空/刺”的主观感受一致，因此更有利于 CNN 学到“人耳在乎的差异”。这也是论文把 CQT 列为三大候选特征之一的动机。
特征二：i‑vector（说话人/信道总变异子空间的固定长度向量）
你可以把一段语音描述成一个“指纹”——一个固定长度的向量，它把说话人特性 + 通道/环境变化都折叠到一个低维空间，这就是 i‑vector。它最初在说话人识别里用得很多。
标准流程（一步步）
帧级特征：通常先抽 MFCC（每帧）。
UBM‑GMM：用很多语音训练一个通用背景高斯混合模型（UBM）。
Baum‑Welch 统计：对目标语音，按 UBM 的各高斯分量累计零/一/二阶统计量，把整段语音汇总成一个很长的超向量 M。
总变异子空间：假设说话人差异与信道差异共同落在某个低秩子空间里，用矩阵 T 把超向量低维投影：
𝑀=𝑚+𝑇𝑤
其中 m 是 UBM 的均值超向量，w 就是我们要的 i‑vector（满足高斯分布）。这篇论文用的是 400 维 i‑vector。
为什么它适合做 MOS 预测？
虽然 i‑vector 不是为“质量”量身定制，但它强烈编码了说话人/通道/环境的信息；而 MOS 恰恰会受这些因素影响（如麦克风、房间混响、网络链路）。因此把 i‑vector 喂给 DNN，有时能得到不错的质量回归基线（本文也做了对比）。
特征三：Mel 特征（MFCC + pitch + VAD + 能量 + 一阶差分 + 上下文拼接）
Mel 频率刻度模仿人耳对不同频段的分辨率：低频更细，高频更粗。MFCC 是在 Mel 滤波器组能量上做倒谱分析得到的“压缩后频谱形状”。
MOS 对“清晰/闷”“嘶/沙”“通透/糊”等感受，和共振峰形状、谱包络变化紧密相关；MFCC 正好在刻画这些。
这篇论文怎么做（非常具体）
分帧：每帧 512 点，步长 160 点，采样率 16 kHz（≈ 32 ms 帧长、10 ms 帧移）。
每帧特征：26 维 MFCC，再拼上：
pitch（基音）、VAD 输出（是否语音帧）、帧对数能量，以及它们的一阶差分（刻画动态）。
上下文：取当前语音帧再加上前后各 12 帧，只保留 VAD 判为有声的帧。
最终输入向量：把这一小段时间窗的所有特征拼成一条大向量，尺寸是 1×1450，再喂给 MLP（全连接 DNN） 去回归 MOS。
为什么它常常效果最好？
MFCC 对谱包络很敏感，配合 pitch/能量/一阶差分能抓住发声状态与瞬态变化；再加上**±12 帧上下文**提供了约 250 ms 的短时动态，这与人对“瞬时质量起伏”的感知窗口相近。本文结果里，Mel+MLP 的相关系数最高（≈0.87），MSE 最低（0.15）。

CNN（配 CQT）：4 层卷积堆叠（前两层 32 个卷积核，第一层核 25×30，后续两层 64×(3×3)），中间有 2×2 池化与 Dropout，最后接 全连接 64 输出 MOS；激活 ReLU；学习率 1e‑4；优化器 Adam。
MLP（配 i‑vector）：输入 400 维，隐层 200/100。
MLP（配 Mel+上下文）：输入 1×1450，4 个全连接隐层，每层 1024 单元；Dropout 0.5，学习率 4e‑4，优化器 Adam。
训练目标/评估：最小化 MSE；报告 Pearson 相关与 MSE。

| 年份   | 论文题目                                                                                               | 是否非侵入      | 应用场景                     | 核心方法/算法                                         | 关键点                                  |
| ---- | -------------------------------------------------------------------------------------------------- | ---------- | ------------------------ | ----------------------------------------------- | ------------------------------------ |
| 2019 | **Intrusive and Non-Intrusive Perceptual Speech Quality Assessment Using CNN**                     | 两种（侵入+非侵入） | VoIP/电信                  | CNN 提取特征 (MFCC+pitch+能量)，支持带参考和无参考 MOS 预测       | 优于 PESQ/POLQA；RMSE 0.35，相关系数 0.89    |
| 2019 | **Non-Intrusive Speech Quality Assessment Using Neural Networks**                                  | 非侵入        | VoIP、在线音频                | CNN / DNN (Mel特征、CQT、i-vector)                  | 基于众包 MOS 标签，Pearson 相关 0.87，MSE 0.15 |
| 2020 | **Neural MOS Prediction for Synthesized Speech Using Multi-Task Learning with Spoofing Detection** | 非侵入        | 合成语音（TTS/VC）             | CNN-BLSTM (MOSNet) + 多任务学习 (Spoofing 检测 + 类型分类) | 提升 11.6%，缓解 MOS 标注主观偏差               |
| 2021 | **MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network**                            | 非侵入        | 合成语音                     | MeanNet + BiasNet (利用个体打分偏差)                    | 捕捉个体听感差异，SRCC 提升 6.7% (VCC2016)      |
| 2021 | **Speech Quality Assessment through MOS using Non-Matching References**                            | 半侵入（非匹配参考） | VoIP/VoLTE               | 使用非匹配参考 (NMR) + wav2vec2 表征                     | 避免严格参考需求，更贴近真实通信场景                   |
| 2021 | **Generalization Ability of MOS Prediction Networks**                                              | 非侵入        | 跨语料 MOS 泛化               | MOSNet、wav2vec2、HuBERT                          | wav2vec2 在零样本场景下泛化最好，支持跨语言 (中/日/英)   |
| 2021 | **DNSMOS Pro: A Reduced-Size DNN for Probabilistic MOS of Speech**                                 | 非侵入        | DNS/VoIP                 | 小型 DNN，端到端预测语音 MOS                              | 为在线降噪优化，计算效率高                        |
| 2021 | **Non-Intrusive Audio Quality Assessment Based on DNN for MOS Prediction**                         | 非侵入        | VoIP/通话质量                | DNN (语音特征 → MOS 回归)                             | 提供可替代 PESQ 的快速评估                     |
| 2021 | **DeePMOS: Deep Posterior Mean-Opinion-Score of Speech**                                           | 非侵入        | VoIP/语音合成                | Bayesian 深度模型 (预测 MOS 后验分布)                     | 提供不确定性度量，增强鲁棒性                       |
| 2023 | **LDNet: Unified Listener Dependent Modeling in MOS Prediction**                                   | 非侵入        | 合成语音                     | Listener-Dependent 网络 (引入听众ID)                  | 引入“虚拟平均听众”，解决打分稀疏和个体差异               |
| 2024 | **EMDSQA: Neural Speech Quality Assessment with Speaker Embedding**                                | 非侵入        | 在线通信 (VoIP, Zoom, Skype) | 自注意力网络 + Speaker Embedding (ECAPA-TDNN, U-Net)  | Pearson r = 0.92，实时推理仅 1.5% CPU 负载   |



---
wav2vec 2.0 的结构分成三大块：

CNN 特征编码器 (Feature Encoder)
输入：原始波形 (16kHz)
输出：低帧率的潜在表示 𝑧𝑡​（每 20ms 一帧）
7 层一维卷积 (Conv1d)，带 LayerNorm 和 GELU 激活。
每层 stride 设置不同，整体下采样率约为 320（16kHz → 50Hz，即 20ms 一帧）。
输出维度：
base 模型：768 维
large 模型：1024 维
作用：代替传统的 MFCC/STFT，把原始波形直接压缩成时序特征。

Transformer 编码器 (Contextual Transformer)
输入：特征编码器输出的序列
输出：带上下文的深层表示 𝑐𝑡​
类似 BERT/Transformer-Encoder 堆叠：
base 模型：12 层，768 维，注意力头数 8，参数约 95M
large 模型：24 层，1024 维，注意力头数 16，参数约 317M
每层包含：
多头自注意力 (Multi-Head Self-Attention)
前馈网络 (FFN)，带 GELU
残差连接 + LayerNorm
作用：建模长时依赖，把局部特征 𝑧𝑡融合成全局表征 𝑐𝑡，捕捉音质/失真等跨时间特征。

量化模块 (Quantization + Contrastive Objective, 训练阶段使用)
用 Gumbel Softmax 把部分表示量化为“代码向量”（离散化）
训练时做对比学习：预测未来帧的正确量化向量，区分干扰负样本

Gumbel Softmax + Codebook：
把 CNN 输出的一部分帧映射到 离散向量（类似词表 embedding）。
这是自监督的关键：让模型预测“未来帧的量化结果”。
对比学习目标 (Contrastive Loss)：
给定上下文向量 𝑐𝑡，预测它未来某时刻的“正确量化向量”，并和一堆负样本对比（InfoNCE loss）。
这样模型学到的特征不仅能重建波形，还捕捉到语义/音质信息。

下采样模块 会把帧特征从 768/1024 维压到 32 维，再给两个任务头。
