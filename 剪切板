特征一：CQT（Constant‑Q Transform，常 Q 变换，用于 CNN）
把语音看成“随时间变化的频率能量图”。传统 STFT 用固定窗口，高频和低频的“分辨率”不一样，跟人耳的频带分辨率并不完全匹配。
CQT 的频带是按几何级数划分，让每个频带的品质因数 Q=中心频率/带宽恒定，更贴近人耳在 500 Hz–20 kHz 的“恒定 Q”感知特性。
这篇论文怎么做
先把一段语音转成 CQT 频谱图（二维图像），再输入 CNN。
为了让 CNN 的输入尺寸固定，作者把频谱帧数统一到 220 帧：短语音就复制最后一帧补齐，长语音就裁掉多余帧；最终尺寸定为 240 × 220（频率 × 时间）。
为什么它适合做 MOS 预测？
CQT 在低频有更高分辨率，对共振峰、混响拖尾、低频噪声更敏感；在高频有更宽带宽，能概括高频粗糙度。
这些都与人对“清晰/闷/空/刺”的主观感受一致，因此更有利于 CNN 学到“人耳在乎的差异”。这也是论文把 CQT 列为三大候选特征之一的动机。
特征二：i‑vector（说话人/信道总变异子空间的固定长度向量）
你可以把一段语音描述成一个“指纹”——一个固定长度的向量，它把说话人特性 + 通道/环境变化都折叠到一个低维空间，这就是 i‑vector。它最初在说话人识别里用得很多。
标准流程（一步步）
帧级特征：通常先抽 MFCC（每帧）。
UBM‑GMM：用很多语音训练一个通用背景高斯混合模型（UBM）。
Baum‑Welch 统计：对目标语音，按 UBM 的各高斯分量累计零/一/二阶统计量，把整段语音汇总成一个很长的超向量 M。
总变异子空间：假设说话人差异与信道差异共同落在某个低秩子空间里，用矩阵 T 把超向量低维投影：
𝑀=𝑚+𝑇𝑤
其中 m 是 UBM 的均值超向量，w 就是我们要的 i‑vector（满足高斯分布）。这篇论文用的是 400 维 i‑vector。
为什么它适合做 MOS 预测？
虽然 i‑vector 不是为“质量”量身定制，但它强烈编码了说话人/通道/环境的信息；而 MOS 恰恰会受这些因素影响（如麦克风、房间混响、网络链路）。因此把 i‑vector 喂给 DNN，有时能得到不错的质量回归基线（本文也做了对比）。
特征三：Mel 特征（MFCC + pitch + VAD + 能量 + 一阶差分 + 上下文拼接）
Mel 频率刻度模仿人耳对不同频段的分辨率：低频更细，高频更粗。MFCC 是在 Mel 滤波器组能量上做倒谱分析得到的“压缩后频谱形状”。
MOS 对“清晰/闷”“嘶/沙”“通透/糊”等感受，和共振峰形状、谱包络变化紧密相关；MFCC 正好在刻画这些。
这篇论文怎么做（非常具体）
分帧：每帧 512 点，步长 160 点，采样率 16 kHz（≈ 32 ms 帧长、10 ms 帧移）。
每帧特征：26 维 MFCC，再拼上：
pitch（基音）、VAD 输出（是否语音帧）、帧对数能量，以及它们的一阶差分（刻画动态）。
上下文：取当前语音帧再加上前后各 12 帧，只保留 VAD 判为有声的帧。
最终输入向量：把这一小段时间窗的所有特征拼成一条大向量，尺寸是 1×1450，再喂给 MLP（全连接 DNN） 去回归 MOS。
为什么它常常效果最好？
MFCC 对谱包络很敏感，配合 pitch/能量/一阶差分能抓住发声状态与瞬态变化；再加上**±12 帧上下文**提供了约 250 ms 的短时动态，这与人对“瞬时质量起伏”的感知窗口相近。本文结果里，Mel+MLP 的相关系数最高（≈0.87），MSE 最低（0.15）。


| 年份   | 论文题目                                                                                               | 是否非侵入      | 应用场景                     | 核心方法/算法                                         | 关键点                                  |
| ---- | -------------------------------------------------------------------------------------------------- | ---------- | ------------------------ | ----------------------------------------------- | ------------------------------------ |
| 2019 | **Intrusive and Non-Intrusive Perceptual Speech Quality Assessment Using CNN**                     | 两种（侵入+非侵入） | VoIP/电信                  | CNN 提取特征 (MFCC+pitch+能量)，支持带参考和无参考 MOS 预测       | 优于 PESQ/POLQA；RMSE 0.35，相关系数 0.89    |
| 2019 | **Non-Intrusive Speech Quality Assessment Using Neural Networks**                                  | 非侵入        | VoIP、在线音频                | CNN / DNN (Mel特征、CQT、i-vector)                  | 基于众包 MOS 标签，Pearson 相关 0.87，MSE 0.15 |
| 2020 | **Neural MOS Prediction for Synthesized Speech Using Multi-Task Learning with Spoofing Detection** | 非侵入        | 合成语音（TTS/VC）             | CNN-BLSTM (MOSNet) + 多任务学习 (Spoofing 检测 + 类型分类) | 提升 11.6%，缓解 MOS 标注主观偏差               |
| 2021 | **MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network**                            | 非侵入        | 合成语音                     | MeanNet + BiasNet (利用个体打分偏差)                    | 捕捉个体听感差异，SRCC 提升 6.7% (VCC2016)      |
| 2021 | **Speech Quality Assessment through MOS using Non-Matching References**                            | 半侵入（非匹配参考） | VoIP/VoLTE               | 使用非匹配参考 (NMR) + wav2vec2 表征                     | 避免严格参考需求，更贴近真实通信场景                   |
| 2021 | **Generalization Ability of MOS Prediction Networks**                                              | 非侵入        | 跨语料 MOS 泛化               | MOSNet、wav2vec2、HuBERT                          | wav2vec2 在零样本场景下泛化最好，支持跨语言 (中/日/英)   |
| 2021 | **DNSMOS Pro: A Reduced-Size DNN for Probabilistic MOS of Speech**                                 | 非侵入        | DNS/VoIP                 | 小型 DNN，端到端预测语音 MOS                              | 为在线降噪优化，计算效率高                        |
| 2021 | **Non-Intrusive Audio Quality Assessment Based on DNN for MOS Prediction**                         | 非侵入        | VoIP/通话质量                | DNN (语音特征 → MOS 回归)                             | 提供可替代 PESQ 的快速评估                     |
| 2021 | **DeePMOS: Deep Posterior Mean-Opinion-Score of Speech**                                           | 非侵入        | VoIP/语音合成                | Bayesian 深度模型 (预测 MOS 后验分布)                     | 提供不确定性度量，增强鲁棒性                       |
| 2023 | **LDNet: Unified Listener Dependent Modeling in MOS Prediction**                                   | 非侵入        | 合成语音                     | Listener-Dependent 网络 (引入听众ID)                  | 引入“虚拟平均听众”，解决打分稀疏和个体差异               |
| 2024 | **EMDSQA: Neural Speech Quality Assessment with Speaker Embedding**                                | 非侵入        | 在线通信 (VoIP, Zoom, Skype) | 自注意力网络 + Speaker Embedding (ECAPA-TDNN, U-Net)  | Pearson r = 0.92，实时推理仅 1.5% CPU 负载   |



---
wav2vec 2.0 的结构分成三大块：

CNN 特征编码器 (Feature Encoder)
输入：原始波形 (16kHz)
输出：低帧率的潜在表示 𝑧𝑡​（每 20ms 一帧）
7 层一维卷积 (Conv1d)，带 LayerNorm 和 GELU 激活。
每层 stride 设置不同，整体下采样率约为 320（16kHz → 50Hz，即 20ms 一帧）。
输出维度：
base 模型：768 维
large 模型：1024 维
作用：代替传统的 MFCC/STFT，把原始波形直接压缩成时序特征。

Transformer 编码器 (Contextual Transformer)
输入：特征编码器输出的序列
输出：带上下文的深层表示 𝑐𝑡​
类似 BERT/Transformer-Encoder 堆叠：
base 模型：12 层，768 维，注意力头数 8，参数约 95M
large 模型：24 层，1024 维，注意力头数 16，参数约 317M
每层包含：
多头自注意力 (Multi-Head Self-Attention)
前馈网络 (FFN)，带 GELU
残差连接 + LayerNorm
作用：建模长时依赖，把局部特征 𝑧𝑡融合成全局表征 𝑐𝑡，捕捉音质/失真等跨时间特征。

量化模块 (Quantization + Contrastive Objective, 训练阶段使用)
用 Gumbel Softmax 把部分表示量化为“代码向量”（离散化）
训练时做对比学习：预测未来帧的正确量化向量，区分干扰负样本

Gumbel Softmax + Codebook：
把 CNN 输出的一部分帧映射到 离散向量（类似词表 embedding）。
这是自监督的关键：让模型预测“未来帧的量化结果”。
对比学习目标 (Contrastive Loss)：
给定上下文向量 𝑐𝑡，预测它未来某时刻的“正确量化向量”，并和一堆负样本对比（InfoNCE loss）。
这样模型学到的特征不仅能重建波形，还捕捉到语义/音质信息。

下采样模块 会把帧特征从 768/1024 维压到 32 维，再给两个任务头。
