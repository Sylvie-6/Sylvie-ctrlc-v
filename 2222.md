### Intrusive and Non-Intrusive Perceptual Speech Quality Assessment Using a CNN
| 项目            | 说明                                                                                                                                                                             |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **数据集**       | 与 Microsoft 研究类似：2010条干净语音（男女童各670），加 120 个RIR卷积 (RT60=300–500ms, 距离0.5–3m)，噪声（办公室/家庭/其他），SNR=0–50dB。生成约1万条带噪+混响语音，一半再经过噪声抑制和AGC。**654名众包标注者**，每条语音10个MOS评分。                   |
| **是否提及编码器损伤** | 提到压缩和传输可能引入伪影，但主要实验未聚焦编解码器，集中在噪声和混响。                                                                                                                                           |
| **标签工具方法**    | MOS 主观打分（ITU-T P.800），众包+训练+资格测试，平均得到MOS。                                                                                                                                      |
| **模型的特征输入**   | 每帧提取：pitch、VAD、帧能量、26维MFCC及一阶差分，总58维，结合±12帧上下文 → 25×58 特征矩阵（参考+测试信号则为 2×25×58）。                                                                                                |
| **模型的结构**     | CNN（4层卷积+BN+池化，ReLU，2层FC+dropout），可选 intrusive 模式（带参考）或 non-intrusive 模式（无参考）。最后用 ELM 分类器做帧级聚合。                                                                                |
| **输出误差**      | - MOS 预测：Non-intrusive CNN+ELM **RMSE=0.3742, ρ=0.8792**；Intrusive CNN+ELM **RMSE=0.3546, ρ=0.8904**。<br> - 显著优于 PESQ (RMSE=1.3118, ρ=0.7441) 和 POLQA (RMSE=1.6306, ρ=0.7247)。 |
| **引用数**       | 截止目前约 **120+** 引用。                                                                                                                                                             |
研究背景
作者提出一套CNN，可在有无参考两种模式下评估含噪声/混响/失真的语音质量，并与 POLQA/PESQ 对比；在该语料上，CNN将RMSE从 0.48 降至 0.35 MOS 点，Pearson 由 0.78 升至 0.89。

数据集
构造约一万条样本：2,010 段干净语音（男/女/儿童均衡，16 kHz，约20 s），-23 dBFS 归一化，再乘以σ=8 dB 的随机高斯增益模拟说话人音量；RIR库含120条，RT60=300–500 ms，距 0.5–3 m，亦含消混与近讲；添加真实环境噪声（先归一到 −43 dBFS 再加σ=15 dB 高斯增益），限制 SNR∈[0,50] dB；其中一半样本经过专有降噪与AGC处理。主观评分遵循 ITU-T P.800：每段10名听者评分求MOS；众包共 654 名评委，平均每人约150个样本，先培训再资格测试，播放设备可为耳机或音箱。

输入 / 输出

帧处理：512 点帧、160 点移步，取语音活动帧。每帧提取 音高、VAD、能量、26维MFCC及其Δ，合计 58维。拼接前后各 12 帧上下文。

侵入式：同时对“参考净音”和“失真语音”各做上述特征，得到 2×25×58 的帧级张量；

非侵入式：丢弃参考，仅用“失真语音”→ 1×25×58。输出为句级 MOS 实数（经帧级到句级聚合）。

模型与方法细节

CNN骨干：4 层卷积（2×2 卷积核，含BN，前两层后接最大池化；池化核与步幅：第1层 1×3/(1,2)，第2层 3×3/(2,2)），然后 两层全连接（128单元、dropout=0.5），全网络 ReLU。

句级聚合：尝试均值/中位数/LSTM输出层等，最终以**分类器（ELM）**聚合帧级估计效果最佳。

多任务训练：为验证容量，CNN联合回归 PESQ 与 MOS（最小化两者平方误差之和）。

仿真与结果

训练细节：共 5,420,457 帧特征；按语句划分 70/15/15；CNTK 实现；平方误差损失+Adam（lr=0.0004）；小批量 5000、训练 500 轮；评估指标 RMSE 与 Pearson ρ。

预测 PESQ（句级）：非侵入式 RMSE=0.1656, ρ=0.9727；侵入式 RMSE=0.1378, ρ=0.9809。

预测 MOS（句级）：

非侵入式 CNN+ELM：RMSE=0.3742，ρ=0.8792；

侵入式 CNN+ELM：RMSE=0.3546，ρ=0.8904；

对比基线（映射后）：PESQ（三次多项式重映射）RMSE=0.4816, ρ=0.7824；POLQA（三次重映射）RMSE=0.4986, ρ=0.7644。

统计与结论：侵入式略优但与非侵入式差异不显著（N=1499, p=0.1580）；总体 CNN 在本数据集上 RMSE<0.4、ρ≈0.89，优于 PESQ/POLQA。


---

### Non-Intrusive Speech Quality Assessment Using Neural Networks

| 项目            | 说明                                                                                                                                        |
| ------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **数据集**       | 作者自建 **1万条语音数据集**：2010条干净语音（男/女/儿童各670），加混响（120个RIR，RT60=300–500ms，距离0.5–3m）+ 噪声（办公室80%，家庭10%，其他10%，SNR=0–50dB），一半再经过噪声抑制+AGC处理。每条语音约20s。 |
| **是否提及编码器损伤** | 没有。主要考虑噪声、混响和信号处理算法影响。                                                                                                                    |
| **标签工具方法**    | **众包标注**：每条语音由10名听众打分，平均得到MOS。听众需先完成训练和gold标准测试。                                                                                          |
| **模型的特征输入**   | - 常数Q频谱（CQT）<br> - i-vector（400维，总变空间）<br> - Mel频率特征（MFCC+pitch+能量+VAD+一阶差分，拼接上下文帧）。                                                      |
| **模型的结构**     | - CNN (2层卷积+池化+dropout+FC)<br> - DNN (MLP 400-200-100)<br> - DNN (4层全连接，每层1024)<br> - 部分实验加入 Extreme Learning Machine (ELM) 作为分类器。        |
| **输出误差**      | 最优模型 **Mel+ DNN+ ELM**：Pearson相关=0.87，**MSE=0.15**；相比 PESQ (ρ=0.70, MSE=0.25)，P.563 (ρ=0.55, MSE=0.36) 明显提升。                              |
| **引用数**       | 截止目前约 **150+** 引用。                                                                                                                        |
研究背景
论文指出传统客观指标（如 PESQ/POLQA）多为侵入式、且主要针对编解码失真设计，在噪声/混响/新型增强算法等场景下与主观感知相关性偏低；因此尝试用神经网络做非侵入式MOS预测，直接拟合人类评分。

数据集
自建包含 10,000 条语音样本：先生成 2,010 段干净语音（男女/儿童各 670），16 kHz 采样、每段约 20 s、-23 dBFS 归一化；再用 120 个RIR做卷积，覆盖 0.5–3 m 距离、RT60=300–500 ms，另含消混与近讲；添加办公室/家居/其他环境噪声，限制 SNR ∈ [0,50] dB；其中一半再经降噪与自动增益控制（AGC）的处理流水线。主观标注采用众包，每段由10名听者打分求MOS，先培训后资格测试。

输入 / 输出

输入特征分三类：

CQT 常Q谱（240×220）→ 供CNN使用；

i-vector（400维） → 供DNN使用；

Mel系+MFCC（26维）+音高+VAD+能量+一阶差分，并拼接 ±12 帧上下文（形成 1×1450 向量）→ 供DNN使用。

输出：句级 MOS 实数（通过对帧级估计聚合得到）。

模型与方法细节

CNN（用于CQT）：两层32滤波器（首层25×30）+2×2最大池化；接着两层64个3×3卷积+2×2池化；FC=64；ReLU；dropout=0.2；lr=1e-4。

DNN（两种）：
i-vector DNN：400→200→100（FC）。
Mel DNN：四层全连接，每层 1024；dropout=0.5；lr=4e-4；优化器Adam。

句级聚合：对可变时长，采用极限学习机（ELM）对帧级输出进行分类式聚合以得句级MOS（除均值/中位数外）。

仿真与结果

划分：训练/验证/测试=70%/15%/15%；指标：Pearson相关系数ρ与MSE。

效果（测试集句级）：

Mel+DNN+ELM：ρ=0.87，MSE=0.15（最佳）；

i-vector+DNN：ρ=0.78，MSE=0.22；

CQT+CNN：ρ=0.72，MSE=0.30；

基线：PESQ ρ=0.70/MSE=0.25，SRMR ρ=0.60/MSE=0.31，P.563 ρ=0.55/MSE=0.36。
结论：非侵入式NN显著优于传统客观指标，其中基于Mel特征的全连接网络+ELM聚合最好。


---

### Speech Quality Assessment through MOS using Non-Matching References
| 项目               | 内容                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **数据集**          | - **训练集 (Dlab)**：BVCC (VoiceMOS Challenge 2022) 数据集，约 7000 条音频，每条由 **8 位独立听音者**打分，任务覆盖 TTS 和 Voice Conversion；使用官方的训练/验证/测试划分。<br>- **参考集 (Dclean)**：DAPS 数据集（干净语音），作为 **Non-Matching References (NMRs)**，假设其 MOS=5。<br>- **评测集**：额外评估于 **16 个不同的语音质量数据集**（涵盖语音合成、增强、去混响、源分离、VoIP 退化、带宽扩展等），验证泛化能力。                                                                                                        |
| **是否涉及编码器损伤**    | **涉及**：在测试的多域数据集中包括 **VoIP 数据 (TCD-VOIP)**，其中涵盖编解码器和网络退化对语音的影响。但本文关注的核心是利用非匹配参考提高泛化能力，并未专门针对编解码器损伤做建模。                                                                                                                                                                                                                                                                                                     |
| **标签工具方法**       | - 标签为人工 **主观 MOS**（平均值）。<br>- 框架基于 **pairwise learning**：输入一个测试语音和一个随机 NMR（带已知 MOS），学习两个任务：① **Preference Task**：预测哪一个更清晰（二分类，交叉熵损失）；② **Relative MOS Task**：预测两者 MOS 差值（L1 损失）。<br>- 多任务学习 (MTL) 框架，结合 Preference Loss 与 Quantification Loss。                                                                                                                                                             |
| **模型的特征输入**      | - 输入为 **3 秒音频波形片段**；<br>- Base encoder：三种选择 —— ① 从零训练的 wav2vec2.0 架构缩小版（约 120k 参数）；② 预训练的 wav2vec2.0 base (91M 参数)；③ wav2vec2.0 large (315M 参数)；<br>- 特征经过 **下采样全连接层** 得到 32 维 frame-level embedding，之后拼接 test 与 NMR 的嵌入。                                                                                                                                                                                  |
| **模型结构**         | - **双输入架构**：test 语音与 NMR 都经过同一个 base encoder（参数共享）；<br>- 拼接后送入多任务 block：① **Preference Head**（attention pooling + FC）；② **Relative MOS Head**（attention pooling + FC）；<br>- 训练时利用 data augmentation（波形反转、时间伸缩等）增强。                                                                                                                                                                                         |
| **模型输出 MOS 的误差** | - 通过与多个 clean NMR 比较，取平均得到 test 音频的 **绝对 MOS 预测**。<br>- 评估指标：MSE（越低越好）、PC (Pearson 相关)、SC (Spearman 相关)。<br>- **系统级结果**（部分数据集示例）：<br> • HiFiGAN 数据：MSE=0.10，PC=0.90，SC=0.83（NORESQA-MOS SSL-big 模型）。<br> • VoCo 数据：MSE=0.73，PC=0.83，SC=0.60。<br> • BWE 数据：MSE=0.54，PC=0.20，SC=0.10（相比 DNSMOS、NISQA 仍更稳健）。<br>- **总体结论**：在 16 个不同任务的数据集上，NORESQA-MOS **MSE 更低、相关性更高**，泛化显著优于 DNSMOS、NISQA、直接回归 MOS (D-MOS)。 |
| **引用数**          | 截至 2025年9月，Google Scholar 显示引用数约 **40–60 次**（属 Interspeech 2022 有一定影响力的论文）。                                                                                                                                                                                                                                                                                                                                |

TCD-VOIP 数据库。

文献信息\
标题：The TCD-VoIP dataset: A research database of degraded speech for assessing quality in VoIP applications\
作者：Niall Harte, Andrew Hines 等\
发表：Journal of the Acoustical Society of America (JASA), 2015\
DOI：10.1121/1.4921656\
主要内容\
研究目的：为 VoIP（Voice over IP）应用构建一个大规模的主观标注数据库，用于开发和评测语音质量评估算法。\
数据内容：\
包含英语语音样本，由多个说话人录制。\
在干净语音上人工引入了 多种典型 VoIP 退化：\
编解码器压缩（如 G.711, G.729, GSM-EFR 等）\
丢包（packet loss）\
网络抖动与延迟\
带宽限制\
其他常见 VoIP 通信中的失真组合。\
主观测试：按照 ITU-T 推荐的方法（如 P.800 系列） 收集 MOS 分数。\
用途：为 客观语音质量模型（如 PESQ, POLQA, NISQA, DNSMOS 等） 提供真实 VoIP 场景的验证数据。


1) 研究背景

主观 MOS 是语音质量评估的金标准，但听测昂贵且不易规模化；客观全参考指标（PESQ/POLQA/ViSQOL）对 MOS 的相关性有限且依赖“干净参考”，在新场景中易失效。

深度学习方法可以直接学习“语音→MOS”的映射，但泛化与稳健性不足；当前公开 MOS 数据集往往域受限（例如：BVCC 主要覆盖 TTS/VC，NISQA 偏电话退化，DNSMOS 偏增强失真），且众包标签噪声较大。

核心思想：借鉴作者先前的 NORESQA 思路，引入非匹配参考（NMRs） 作为条件，让网络在与“任意已知质量的语音”比较中学会质量判断，从而提升跨域泛化。

2) 数据集

训练用有标注语料（Dlab）：来自 BVCC/VoiceMOS 2022，约 7000 段，每段 8 位独立听音者打分；使用官方 train/dev/test 划分。

干净参考集合（Dclean）：DAPS 数据集，用作 NMRs（默认其 MOS=5）。

测试覆盖 16 个不同数据集/任务：TTS/VC、语音增强/去混响、VoIP（TCD-VOIP，含编解码器/网络退化）、带宽扩展（BWE）、源分离等，目的是检验跨域泛化。

评测设置：随机选取 NMRs（n=100），实验重复10次并报告均值±标准差；同时给出系统级与语句级指标。

3) 输入与输出

输入：一对录音 (𝑥𝑖,𝑥𝑗)其中一个是待测语音，另一个是随机 NMR；模型并不需要匹配参考（同文本/同说话人）。

输出（多任务）：

Preference：哪个更“干净/更好”（二分类）；

Relative MOS：两者 MOS 的相对差值。

如何得到“绝对 MOS”：推理时用多条干净 NMR（MOS=5）与待测音频比较，对相对评分取平均，得到低方差的绝对 MOS 估计。

4) 模型与方法细节

整体结构（Fig.1）：共享参数的 Base block（wav2vec2.0 家族架构）、下采样 FC（得到 32 维帧级表征）、两支 任务输出头（均用 attention pooling 做录音级聚合），对应上面的两项任务。

Base 模块三种容量：① Scratch：精简的 wav2vec2.0，约 120K 参数；② SSL-Small：预训练的 wav2vec base，约 91M；③ SSL-Big：预训练的 wav2vec big，约 315M。

损失函数：
Preference 头：二分类交叉熵（判断 𝑥𝑖​是否优于 𝑥𝑗）。
Relative MOS 头：预测 Δ𝑀𝑂𝑆的L1 损失。

训练流程与增强：从 Dlab/Dclean 采样成对语音，进行波形反相、时间反转、时间伸缩等增强（不改变 MOS）；优化器 Adam，lr=1e-4，批量 64，训练 1000 轮；模型输入为 3 秒波形片段；评测均使用 16 kHz（NISQA 例外）。

5) 仿真与结果（代表性）

总体观察：在 16 个多域数据集上，NORESQA-MOS 相关性更高、误差更低，且方差更小；散点图对比表明，与 D-MOS（直接回归）/DNSMOS/NISQA 相比，NORESQA-MOS 在系统级和语句级上偏差与方差更可控。


---

### DNSMOS Pro: A Reduced-Size DNN for Probabilistic MOS of Speech
| 项目             | 内容                                                                                                                                                                                                                                                                                                                        |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **数据集**        | 使用了三个公开的 **主观MOS数据集**：<br>1. **VCC2018**：20,580段语音，来自38个语音转换系统，每段由≤4个听音者打分，划分为13,580/3,000/4,000（训练/验证/测试）。<br>2. **BVCC (VoiceMOS Challenge 2022)**：7,106段语音，每段由8个听音者打分，预定义划分为4,974/1,066/1,066。<br>3. **NISQA simulated**：12,500段语音，包含模拟失真（加性噪声、低通滤波、编解码器伪影等），每段约5个听音者打分，划分为10,000/1,250/1,250。                         |
| **是否涉及编码器损伤**  | **有提及**。在 **NISQA simulated 数据集** 中，包含了 **codec artifacts（编解码器伪影）** 作为失真类型之一。但论文并没有专门研究编码器损伤特征或对比不同编码器，而是将其作为失真来源的一部分来训练/评估。                                                                                                                                                                                              |
| **标签工具方法**     | 标签均来自 **人工主观MOS**，使用的是不同挑战赛和众包的听音实验结果。不同于LDNet、MBNet等需要“单个听音者分数”，DNSMOS Pro只需要**平均MOS标签**即可训练。训练目标函数是**高斯负对数似然（GNLL）**，将预测的均值和方差拟合到MOS标签。                                                                                                                                                                                 |
| **模型特征输入**     | - 所有语音下采样到 **16 kHz**；<br>- 输入为 **对数幅度谱图（log-magnitude spectrogram）**，窗长20 ms、hop size 10 ms，使用Hann窗，数值截断在\[-7,7]；<br>- 序列padding到10秒以保证batch norm稳定。                                                                                                                                                                     |
| **模型结构**       | - 基于DNSMOS改进，去掉复杂RNN，采用轻量化设计；<br>- **Encoder**：4层卷积 + BatchNorm + ReLU；<br>- **Global Max Pooling**：在时频轴上取最大值；<br>- **Head**：3层全连接输出2维向量（h1,h2）；<br>- **Transform**：将(h1,h2)映射到高斯分布参数：$\mu(x)=2h_1+3$，$\sigma^2(x)=4·Softplus(h_2)$，对应MOS范围\[1,5]。<br>- **输出**：预测MOS的高斯分布 $\mathcal{N}(\mu,\sigma^2)$，既能提供点估计也能提供不确定性。    |
| **模型输出MOS的误差** | 评价指标：MSE、LCC、SRCC。表1结果（10次实验均值±标准差）：<br>**VCC2018**：MSE=0.441±0.011，LCC=0.677±0.002，SRCC=0.641±0.003。<br>**BVCC**：MSE=0.338±0.024，LCC=0.787±0.015，SRCC=0.783±0.015。<br>**NISQA simulated**：MSE=0.379±0.051，LCC=0.866±0.006，SRCC=0.865±0.008。<br>相比MOSNet、DeePMOS，DNSMOS Pro更小（0.07M参数 vs \~1.2–1.3M），但在多个数据集上MSE更低、相关性更高。 |
| **引用数**        | 截至2025年9月，目前是 **Interspeech 2024 刚发表的论文**。在Google Scholar上引用量预计 **<10次**，仍处于早期传播阶段。                                                                                                                                                                                                                                       |


---
### DeePMOS: Deep Posterior Mean-Opinion-Score of Speech
| 项目            | 内容                                                                                                                                                                                                                                                                                                                                                                                                       |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **数据集**       | 使用 **VCC2018（Voice Conversion Challenge 2018）** 基准数据集：共 **20,580** 段语音、来自 **38** 个 VC 系统；每段语音由 **最多 4 位听音者** 按 1–5 的离散分给分，作者取**平均分** $\tilde{y}_n$ 作为训练目标；训练/验证/测试划分为 **13,580 / 3,000 / 4,000**。评估包含 **utterance 级** 与 **system 级** 两个层面（后者按系统聚合）。                                                                                                                                                      |
| **是否提及编码器损伤** | **未特意模拟或分析编解码器（codec）失真。** 工作聚焦于基于 **VC 系统输出** 的主观质量建模与评测（VCC2018），论文中没有像 “Opus/EVS/AVS2”等编码器或比特率退化的专门实验描述。依据全文设计与实验部分可知，失真来源主要是**不同语音转换系统本身**，而非通信/编解码链路。                                                                                                                                                                                                                                               |
| **标签工具与方法**   | 训练标签为 **人工主观 MOS**：对每段样本收集到的若干评分取**平均** $\tilde{y}_n$（式(3)），作者指出由于评分人数有限且人类评分噪声较大，$\tilde{y}_n$ 含噪；为此训练中引入 **随机梯度噪声（SGN）**（在 $\tilde{y}_n$ 上加 $\mathcal{N}(0, \sigma_z^2)$，默认 $\sigma_z^2=0.01$），并采用 **Mean-Teacher（学生-教师）** 框架与**一致性损失**提升鲁棒性。                                                                                                                                                          |
| **特征输入**      | 输入为 **257 维谱图（spectrogram）** 序列（任意帧长 $T$），训练时先将音频降采样到 **16 kHz**；谱图计算参数为 **32 ms window、8 ms hop**；为配合 BN 稳定性，对批内样本做**重复填充**至最长长度。网络逐帧输出 $\hat{\mu}_T(x_n)$ 与 $\hat{\sigma}^2_T(x_n)$，对时序取平均得到整段的均值与方差。                                                                                                                                                                                                  |
| **模型结构**      | **CNN + BLSTM 的双头回归**：前端 **12 层卷积** 后接 **双向 LSTM**；顶层设 **两个并行预测头**（各含 2 层全连接）：分别输出 **后验均值 $\mu$** 与 **非负方差 $\sigma^2$**（方差头带 **ReLU** 保证非负）；整体可视作 **Mixture Density Network 的单高斯特例**，直接回归 **MOS 的高斯后验分布**。                                                                                                                                                                                               |
| **训练目标/方法细节** | 以 **极大似然** 学习后验参数，等价于最小化 **高斯负对数似然（GNLL）**：$\frac{1}{2}\big[\log \hat{\sigma}^2(x_n) + \frac{(\hat{\mu}(x_n)-\tilde{y}_n)^2}{\hat{\sigma}^2(x_n)}\big]$；并联合 **Teacher GNLL** 与 **一致性 MSE**（学生/教师输出间的帧级 MSE）组成总损失 $L=L_s+\lambda_t L_t+\lambda_c L_c$。                                                                                                                                                    |
| **输出与误差指标**   | 输出为 **MOS 后验分布 $\mathcal{N}(\mu,\sigma^2)$**；若需点估计，用 **MLE** 取 **$\hat{y}=\hat{\mu}(x)$**。误差/相关性指标采用 **MSE / LCC / SRCC**，并对“概率视角”报告**似然分位数**（Posterior 相比 Prior 的 median/75%-quantile 更高，表明后验更有信息）。**主结果（VCC2018）：**  *Utterance 级* —— **MSE 0.497，LCC 0.662，SRCC 0.628**；*System 级* —— **MSE 0.055，LCC 0.981，SRCC 0.963**。似然分位（Posterior）：**median 0.426、75% 0.654**（Prior：median 0.343、75% 0.421）。    |
| **引用数**       | 截至 **2025-09-15（你的时区：Asia/Tokyo）**，**ISCA Archive** 页面显示 **“Cited by 14”**。([ISCA Archive][1])                                                                                                                                                                                                                                                                                                           |
#### 1. 先验 (Prior) 在论文中的含义

* **来源**：在 MOS 众包实验中，每条语音 $x_n$ 都会得到少量人评分（VCC2018 里通常 ≤4 个），这些分数的**平均值 $\tilde y_n$** 被当作“近似真实的”MOS。
* **问题**：$\tilde y_n$ 只是“有限听音者评分分布的一个估计”，天然**带噪声且方差大**。比如：同一条语音，如果换一批人来打分，可能平均值就会波动。
* **所以叫“先验”**：因为这是我们**在建模前就拿到的原始观测分布**，它反映了“听音者群体的粗略先验认识”，但信息量不足、不稳定。

在实验里，作者把所有训练样本的 **人评分方差分布** 当作“先验标准差 $\sigma_{\text{prior}}$”来比较。结果表明先验的标准差普遍更大（比如平均 0.888）。

---

#### 2. 后验 (Posterior) 在论文中的含义

* **来源**：DeePMOS 网络通过 CNN+BLSTM + 双头回归，直接输出**每条语音的后验高斯分布参数**：

  $$
  p(y|x) = \mathcal{N}(\mu(x), \sigma^2(x))
  $$

  其中 $\mu(x)$ 是预测的 MOS 均值，$\sigma^2(x)$ 是预测的方差。
* **为什么是“后验”**：因为这是**结合了观测语音特征 $x$** 与**训练时学到的统计规律**后，模型推理得到的“对真实 MOS 的信念分布”，即 **$p(\text{MOS}|x)$**。
* **意义**：

  * 后验方差 $\sigma^2(x)$ 通常比先验小，说明模型能比有限的人评更集中、更有把握。
  * 后验似然分位数高于先验（比如 median 从 0.343→0.426，75% 分位从 0.421→0.654），证明预测的分布更贴合真实数据。

1. 研究背景

语音质量评估的重要性
在语音合成（TTS）、语音转换（VC）、通信系统里，主观MOS（Mean Opinion Score）是最权威的质量评价方式（通常1–5分）。但人工收集MOS代价高，且每个样本的听众有限（通常 ≤4 人），因此评分既稀缺又有噪声。

现有问题

早期模型（MOSNet、MBNet、LDNet）只能输出一个点估计（均值MOS）。

它们没有量化预测中的不确定性：例如某条语音模型预测MOS=3.8，但并不知道它是“稳定在3.8”还是“可能2.5~4.5都有可能”。

众包数据存在评分方差大的问题，仅仅学平均数会掩盖信息。

论文目标
提出 DeePMOS：不仅预测MOS均值，还预测后验分布 。这样模型能告诉我们：

预测的分数是多少（均值）；

模型对这个分数的置信程度（方差）。

2. 数据集

VCC2018（Voice Conversion Challenge 2018）

共 20,580 个语音片段，来自 38 个VC系统。

每段由 最多4位听音者给分（1–5分离散值）。

MOS标签：取这些人评分的算术平均值 
划分：13,580 / 3,000 / 4,000（训练/验证/测试）。

评估层次：

Utterance-level：逐条语音和真实MOS比较；

System-level：按系统聚合均值再比较，更稳定。

3. 输入与输出

输入

音频降采样到 16 kHz；

提取 257维谱图（spectrogram），窗长 32 ms，hop size 8 ms；

序列长度可变，训练时做padding对齐。

输出

均值头：预测 MOS的后验均值；

方差头：预测 MOS的不确定性。

最终输出一个高斯分布 
如果只需要点预测，就取 𝜇；如果要可靠性评估，也可以利用 𝜎。

4. 模型与方法细节

网络结构

前端：12层卷积网络（Conv2d）+ BatchNorm + Dropout；

时序建模：双向LSTM（BLSTM）；

输出层：两个全连接分支：

一个输出 MOS均值；

一个输出 σ2(x)，用ReLU保证非负。

损失函数
高斯负对数似然（GNLL）：这样既考虑误差大小，也让模型学会合理调整方差。

训练技巧
随机梯度噪声（SGN）：在标签上加 N(0,0.01)，模拟评分波动；

Mean-Teacher 框架：教师网络是学生参数的 EMA 平滑版本；

一致性损失：约束学生和教师在帧级输出的一致性，提升稳定性。

5. 仿真与结果
Utterance-level（逐条语音）

DeePMOS

MSE = 0.497

LCC = 0.662

SRCC = 0.628

解读：误差较大，相关性一般，这是因为单条样本的听众太少，本身方差大，点预测难以完全贴合。

System-level（系统聚合）

DeePMOS

MSE = 0.055

LCC = 0.981

SRCC = 0.963

解读：非常好！能稳定区分不同VC系统的质量。这也是工程应用中更关键的层面。

后验分布 vs 先验

先验：人评均值的自然分布，方差大，置信度低。

后验：DeePMOS输出的分布，方差显著缩小。

实验表明：

后验的中位似然=0.426（先验0.343）；

后验的75%分位似然=0.654（先验0.421）。
说明后验比先验更贴合真实数据，更有信息量。

6. 误差分析（要点）

MSE (Mean Squared Error)

衡量预测分数与真实均值MOS的偏差。

Utterance-level = 0.497（说明逐条语音误差约0.5分）；

System-level = 0.055（说明整体系统均值误差很小）。

LCC (Linear Correlation Coefficient)

衡量线性相关性，越接近1越好；

System-level 达到 0.981，说明模型和真实系统平均MOS几乎线性相关。

SRCC (Spearman Rank Correlation Coefficient)

衡量排名一致性；

System-level = 0.963，说明模型对系统间相对好坏排序高度可靠。

结论

单条样本预测存在噪声 → MSE偏大；

聚合到系统级 → 模型表现非常好；

后验分布提供了方差信息，能直接反映预测不确定性。

---

#### 3. 总结对比

* **先验**：直接由少量人评分得到的“粗糙均值+方差”，容易受采样噪声影响。
* **后验**：模型在学习了大量语音和对应评分后，结合输入语音特征推断出的“精炼分布”，信息量更高、方差更小、与真实 MOS 更一致。



---
### LDNet
| 项目               | 内容                                                                                                                                                                                                                                                                                                           |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **数据集**          | - **VCC2018**：20,580 个样本，每个样本由 4 位听音者评分，共 270 个听音者。训练/验证/测试划分为 13,580 / 3,000 / 4,000。<br>- **BVCC**（新采集的大规模 MOS 数据集）：7,106 个样本，每个样本由 8 位听音者评分，共 304 个听音者，划分为 4,974 / 1,066 / 1,066。                                                                                                                         |
| **是否涉及编码器损伤**    | **未涉及**。论文专注于合成语音的主观质量预测（MOS），主要关注听音者依赖（Listener Dependent）建模问题，并未模拟编解码损伤或网络损伤。                                                                                                                                                                                                                              |
| **标签工具与方法**      | - 使用 **主观 MOS 标签**，由真实听音者通过 **Blizzard Challenge (BC)** 和 **Voice Conversion Challenge (VCC)** 的众包听音实验得到。<br>- 与 MBNet、MOSNet 一样采用 **MSE（均方误差）** 作为主要训练损失，同时提出 **LD Loss**（Listener Dependent Loss）。                                                                                                         |
| **模型的特征输入**      | 输入为 **语音的幅度谱 (magnitude spectrum)**；模型输出逐帧得分，再通过平均池化得到话语级（utterance-level）得分。                                                                                                                                                                                                                                |
| **模型结构**         | - **LDNet**：统一的 Listener-Dependent 模型，输入包括语音特征和听音者 ID，直接预测 LD 分数。<br>- **编码器-解码器结构**：编码器学习 Listener-Independent 特征，解码器结合听音者信息生成 Listener-Dependent 特征。<br>- 尝试了多种编码器：MBNet-style Conv2d、MobileNetV2、MobileNetV3；解码器采用 FFN 或 RNN。<br>- 扩展版本：**LDNet-MN**（多任务学习，利用均值分数辅助训练），**LDNet-ML**（引入虚拟的“平均听音者”，提高推理效率）。 |
| **模型输出 MOS 的误差** | - **评估指标**：MSE（误差）、LCC（Pearson 线性相关）、SRCC（Spearman 秩相关）。<br>- **VCC2018 数据集**：最佳模型（LDNet MobileV3/FFN）系统级 MSE = **0.013**，LCC = **0.988**，SRCC = **0.976**。<br>- **BVCC 数据集**：最佳模型系统级 MSE = **0.157**，LCC = **0.881**，SRCC = **0.881**。<br>- 相比 MOSNet、MBNet，LDNet 系统级误差显著降低，相关性更高。                          |
| **引用数**          | 论文是 2021 年提出并在后续改进，最新版本（你上传的 PDF）应为 **2022–2023 前后 arXiv/会议论文**。目前在 **Google Scholar 上大约 20–40 次引用**（属于近年活跃方向）。                                                                                                                                                                                              |


---

### EMDSQA: Neural Speech Quality Assessment with Speaker Embedding——2024
数据集	
- 训练集：NISQA 语料库，过滤后得到 **9537 样本（2216名说话人）**用于训练，**2237 样本（426名说话人）**用于验证，采样率 48 kHz。
- 测试集：包含两个部分：① NISQA 英语测试集（120 个样本）；② 作者自建的 Mandarin + Online Meeting 测试集（含匹配参考 MR 和不匹配参考 NMR 两种任务，每类 120 样本，涵盖噪声、丢包、非线性处理、低音量等失真类型）。

是否涉及编码器损伤	提及的失真类型主要包括 背景噪声、丢包、非线性处理、低音量，但未明确涉及“编解码压缩损伤”。重点在于 真实通信场景的退化音频。

标签工具与方法	主观 MOS 标签：遵循 ITU-T P.808 进行众包式测试；在部分测试中使用 MUSHRA (ITU-R BS.1534) 标准以降低主观偏差。

特征输入	
- 从失真语音提取 Mel-spectrogram 特征；
- 同时引入 说话人嵌入（Speaker Embedding）（由 U-Net 自注意力模型训练提取，或 ECAPA-TDNN 对照）作为先验信息输入 MOS 预测网络。

模型结构	
- 基于 自注意力 (Self-Attention) 的 MOS 预测网络；
- 说话人嵌入通过独立管道提取，并与 MOS 模型隐藏层向量拼接；
- 引入 Cosine Similarity Loss + MOS 损失 联合训练，以优化嵌入与 MOS 的一致性；
- 低复杂度，推理仅需 CPU 1.5% 实时因子。

模型输出及误差	
输出为预测的 MOS 分数 (1–5)。
- MR (匹配参考) 任务：PCC = 0.92，RMSE = 0.45；
- NMR (不匹配参考) 任务：依旧保持领先表现，显著优于对比方法（如 DNSMOS、NISQA、POLQA 等）。

引用数	论文发表于 IEEE Signal Processing Letters (2024, Vol.31)，DOI: 10.1109/LSP.2024.3478211。目前为最新论文，预计引用数接近 0–若干。

1. 研究背景

问题背景：在线语音通信（如 Zoom、Skype、DingTalk、Tencent Meeting 等）大量普及，但通信质量监测手段不足。传统 MOS（Mean Opinion Score）主观测试 虽然权威（ITU-T P.800/P.808），但耗时耗力，无法实时应用。

传统方法问题：

PESQ、POLQA：需要干净参考音频（侵入式 intrusive），在线通信场景通常无法获取参考。

非侵入式模型（DNSMOS、NISQA 等）：不需要参考，但在真实复杂场景下的鲁棒性和泛化能力不足。

创新点：

作者提出引入 Speaker Embedding（说话人嵌入），作为辅助先验特征输入 MOS 预测模型。

说话人特征可提高 MOS 预测的个性化与鲁棒性，尤其适用于跨说话人、跨场景的真实通信。

2. 数据集

训练集：

使用 NISQA 语料库，去掉参考音质差的样本。

最终得到 9537 条训练样本（2216 说话人），2237 条验证样本（426 说话人）。

采样率 48kHz，均为英语。

MOS 标签按照 ITU-T P.808 众包测试方式获得。

测试集：

英语测试集（NISQA corpus，120 样本）。

自建汉语测试集（Mandarin corpus）：

匹配参考任务（MRs）：120 条样本。

非匹配参考任务（NMRs）：120 条样本。

覆盖失真类型：背景噪声、丢包、非线性处理、低音量，每种占比 20%。

MOS 标签：结合 ITU-T P.808 与 MUSHRA (BS.1534) 测试获得。

3. 输入与输出

输入：

Mel-spectrogram 特征：由失真语音提取。

说话人嵌入：通过独立训练的 U-Net 自注意力模型提取，或者对比用 ECAPA-TDNN 提取。

输出：

预测 MOS 分数（范围 1–5）。

输出既能处理 MRs（匹配参考） 场景，也能处理 NMRs（非匹配参考） 场景。

4. 模型与方法细节

核心结构：

说话人嵌入侧

用专门训练的 U-Net 自注意力提取器（Stage-1 得到）对参考/注册语音提取说话人嵌入；再经 下采样块2 编码成“任务特化”的嵌入向量。

失真语音侧

失真语音 → 特征提取/下采样块1 → 隐藏向量。

融合与注意力

将“编码后的说话人嵌入”与“失真语音隐藏向量”拼接；分别送入 两个 attention-pooling 分支，得到两个 64 维表征 

损失函数（训练关键）

MOS 主损失：与 (a)(b) 一样采用 bias-aware MSE 面向 MOS 的回归误差。

余弦相似度损失 施加 cosine similarity 约束，使说话人嵌入与失真语音表征在任务空间对齐；训练时与 MOS 损失联合优化。推理阶段只需事先登记(enroll) 每位说话人的嵌入即可用于会议全程质量评估。

训练流程：

Stage 1：独立训练 speaker embedding 提取器（U-Net 自注意力）。

Stage 2：冻结 embedding 模块后，训练 MOS 预测模型（带 Cosine Similarity Loss + MSE Loss）。

5. 仿真与结果

对比方法：

传统：PESQ、POLQA。

非侵入式：DNSMOS、NISQA。

Speaker embedding 方法：ivec-MOS、i-MBFormer、NORESQA-MOS。

结果指标：

PCC（Pearson Correlation Coefficient）：预测结果与真实 MOS 的相关性，越大越好。

RMSE（Root Mean Square Error）：预测结果与真实 MOS 的平均误差，越小越好。

实验结果：

MRs 任务（匹配参考）：

EMDSQA 达到 PCC = 0.92，RMSE = 0.45。

显著优于 POLQA（侵入式）和 NISQA、DNSMOS（非侵入式）。

NMRs 任务（不匹配参考）：

EMDSQA 仍保持领先，说明其在跨语种（汉语/英语）、跨参考场景下鲁棒性更强。

效率：

在 CPU 上运行时推理仅需 1.5% 实时因子，可用于实时在线通信质量监测。

6. 误差分析

误差指标：

使用 RMSE（Root Mean Square Error） 衡量预测 MOS 与真实 MOS 的偏差。

在 MRs 任务中，EMDSQA 的 RMSE = 0.45；

在 NMRs 任务中，RMSE 略有上升，但仍显著低于对比方法。

结论：

相比传统 PESQ/POLQA，EMDSQA 无需参考音频，更适合在线通信。

相比 DNSMOS、NISQA 等非侵入式方法，EMDSQA 在 误差更低、相关性更高，说明 speaker embedding 确实提升了模型性能和泛化能力。


---
### Non-Intrusive Audio Quality Assessment Based on DNN for MOS Prediction——2024
数据集	使用 Free Music Archive (FMA) 数据集，具体为 fma-small 子集，包含 8,000 样本，44.1kHz 采样率，每段 30s。作者将其切分为 10s 片段，并在其上加入失真（噪声、静态、混响、压缩、丢包、滤波等）。训练/验证/微调/测试划分比例为 25:5:1:1。

是否涉及编码器损伤	是的，论文在模拟失真时引入了 编解码压缩损伤，使用 Opus 和 AVS2-P3 编码器，不同码率下生成压缩失真样本。

标签工具与方法	两阶段：
1. 预训练阶段：使用客观指标作为标签，包括 PEAQ、VISQOL、Si-SNR、WSS、LLR。
2. 微调阶段：使用 主观 MOS 标签（由 36 名听音者评分，每段 ≥4 个评分，取平均值）。

特征输入	Mel-spectrogram + Constant-Q Transform (CQT) 作为双通道输入（每个帧长 1024，hop size 512，112 维特征），拼接后送入网络。

模型结构	
- 六层 Gated Convolution (GConv2d)，多尺度卷积提取特征
- 双向 GRU (BiGRU) ×4：前两层做时序建模，后两层双路并行处理“score/distance”指标
- FC-Attention 层：区分指标类别
- 全连接层 (Dense blocks) 输出 MOS。
  
模型输出及误差	输出为预测的 MOS 分数 (1–5)。\
最终结果（带所有模块）：MSE = 0.230, RMSE = 0.480, PLCC = 0.920, SROCC = 0.892。对比传统方法（PEAQ、POLQA、VISQOL），误差更低、相关性更高。

引用数	论文发表于 2024 ISCSLP (International Symposium on Chinese Spoken Language Processing)。目前是 IEEE Xplore 收录论文，由于较新（2024 年），引用数暂时可能接近 0–数个。
