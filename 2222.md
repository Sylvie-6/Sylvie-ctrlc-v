数据集	使用 Free Music Archive (FMA) 数据集，具体为 fma-small 子集，包含 8,000 样本，44.1kHz 采样率，每段 30s。作者将其切分为 10s 片段，并在其上加入失真（噪声、静态、混响、压缩、丢包、滤波等）。训练/验证/微调/测试划分比例为 25:5:1:1。

是否涉及编码器损伤	是的，论文在模拟失真时引入了 编解码压缩损伤，使用 Opus 和 AVS2-P3 编码器，不同码率下生成压缩失真样本。

标签工具与方法	两阶段：
1. 预训练阶段：使用客观指标作为标签，包括 PEAQ、VISQOL、Si-SNR、WSS、LLR。
2. 微调阶段：使用 主观 MOS 标签（由 36 名听音者评分，每段 ≥4 个评分，取平均值）。

特征输入	Mel-spectrogram + Constant-Q Transform (CQT) 作为双通道输入（每个帧长 1024，hop size 512，112 维特征），拼接后送入网络。

模型结构	- 六层 Gated Convolution (GConv2d)，多尺度卷积提取特征
- 双向 GRU (BiGRU) ×4：前两层做时序建模，后两层双路并行处理“score/distance”指标
- FC-Attention 层：区分指标类别
- 全连接层 (Dense blocks) 输出 MOS。
- 
模型输出及误差	输出为预测的 MOS 分数 (1–5)。\
最终结果（带所有模块）：MSE = 0.230, RMSE = 0.480, PLCC = 0.920, SROCC = 0.892。对比传统方法（PEAQ、POLQA、VISQOL），误差更低、相关性更高。

引用数	论文发表于 2024 ISCSLP (International Symposium on Chinese Spoken Language Processing)。目前是 IEEE Xplore 收录论文，由于较新（2024 年），引用数暂时可能接近 0–数个。
