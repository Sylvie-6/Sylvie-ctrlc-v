### DeePMOS: Deep Posterior Mean-Opinion-Score of Speech
| 项目            | 内容                                                                                                                                                                                                                                                                                                                                                                                                       |
| ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **数据集**       | 使用 **VCC2018（Voice Conversion Challenge 2018）** 基准数据集：共 **20,580** 段语音、来自 **38** 个 VC 系统；每段语音由 **最多 4 位听音者** 按 1–5 的离散分给分，作者取**平均分** $\tilde{y}_n$ 作为训练目标；训练/验证/测试划分为 **13,580 / 3,000 / 4,000**。评估包含 **utterance 级** 与 **system 级** 两个层面（后者按系统聚合）。                                                                                                                                                      |
| **是否提及编码器损伤** | **未特意模拟或分析编解码器（codec）失真。** 工作聚焦于基于 **VC 系统输出** 的主观质量建模与评测（VCC2018），论文中没有像 “Opus/EVS/AVS2”等编码器或比特率退化的专门实验描述。依据全文设计与实验部分可知，失真来源主要是**不同语音转换系统本身**，而非通信/编解码链路。                                                                                                                                                                                                                                               |
| **标签工具与方法**   | 训练标签为 **人工主观 MOS**：对每段样本收集到的若干评分取**平均** $\tilde{y}_n$（式(3)），作者指出由于评分人数有限且人类评分噪声较大，$\tilde{y}_n$ 含噪；为此训练中引入 **随机梯度噪声（SGN）**（在 $\tilde{y}_n$ 上加 $\mathcal{N}(0, \sigma_z^2)$，默认 $\sigma_z^2=0.01$），并采用 **Mean-Teacher（学生-教师）** 框架与**一致性损失**提升鲁棒性。                                                                                                                                                          |
| **特征输入**      | 输入为 **257 维谱图（spectrogram）** 序列（任意帧长 $T$），训练时先将音频降采样到 **16 kHz**；谱图计算参数为 **32 ms window、8 ms hop**；为配合 BN 稳定性，对批内样本做**重复填充**至最长长度。网络逐帧输出 $\hat{\mu}_T(x_n)$ 与 $\hat{\sigma}^2_T(x_n)$，对时序取平均得到整段的均值与方差。                                                                                                                                                                                                  |
| **模型结构**      | **CNN + BLSTM 的双头回归**：前端 **12 层卷积** 后接 **双向 LSTM**；顶层设 **两个并行预测头**（各含 2 层全连接）：分别输出 **后验均值 $\mu$** 与 **非负方差 $\sigma^2$**（方差头带 **ReLU** 保证非负）；整体可视作 **Mixture Density Network 的单高斯特例**，直接回归 **MOS 的高斯后验分布**。                                                                                                                                                                                               |
| **训练目标/方法细节** | 以 **极大似然** 学习后验参数，等价于最小化 **高斯负对数似然（GNLL）**：$\frac{1}{2}\big[\log \hat{\sigma}^2(x_n) + \frac{(\hat{\mu}(x_n)-\tilde{y}_n)^2}{\hat{\sigma}^2(x_n)}\big]$；并联合 **Teacher GNLL** 与 **一致性 MSE**（学生/教师输出间的帧级 MSE）组成总损失 $L=L_s+\lambda_t L_t+\lambda_c L_c$。                                                                                                                                                    |
| **输出与误差指标**   | 输出为 **MOS 后验分布 $\mathcal{N}(\mu,\sigma^2)$**；若需点估计，用 **MLE** 取 **$\hat{y}=\hat{\mu}(x)$**。误差/相关性指标采用 **MSE / LCC / SRCC**，并对“概率视角”报告**似然分位数**（Posterior 相比 Prior 的 median/75%-quantile 更高，表明后验更有信息）。**主结果（VCC2018）：**  *Utterance 级* —— **MSE 0.497，LCC 0.662，SRCC 0.628**；*System 级* —— **MSE 0.055，LCC 0.981，SRCC 0.963**。似然分位（Posterior）：**median 0.426、75% 0.654**（Prior：median 0.343、75% 0.421）。    |
| **引用数**       | 截至 **2025-09-15（你的时区：Asia/Tokyo）**，**ISCA Archive** 页面显示 **“Cited by 14”**。([ISCA Archive][1])                                                                                                                                                                                                                                                                                                           |

[1]: https://www.isca-archive.org/interspeech_2023/liang23d_interspeech.html?utm_source=chatgpt.com "DeePMOS: Deep Posterior Mean-Opinion-Score of Speech"



---
### LDNet
| 项目               | 内容                                                                                                                                                                                                                                                                                                           |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **数据集**          | - **VCC2018**：20,580 个样本，每个样本由 4 位听音者评分，共 270 个听音者。训练/验证/测试划分为 13,580 / 3,000 / 4,000。<br>- **BVCC**（新采集的大规模 MOS 数据集）：7,106 个样本，每个样本由 8 位听音者评分，共 304 个听音者，划分为 4,974 / 1,066 / 1,066。                                                                                                                         |
| **是否涉及编码器损伤**    | **未涉及**。论文专注于合成语音的主观质量预测（MOS），主要关注听音者依赖（Listener Dependent）建模问题，并未模拟编解码损伤或网络损伤。                                                                                                                                                                                                                              |
| **标签工具与方法**      | - 使用 **主观 MOS 标签**，由真实听音者通过 **Blizzard Challenge (BC)** 和 **Voice Conversion Challenge (VCC)** 的众包听音实验得到。<br>- 与 MBNet、MOSNet 一样采用 **MSE（均方误差）** 作为主要训练损失，同时提出 **LD Loss**（Listener Dependent Loss）。                                                                                                         |
| **模型的特征输入**      | 输入为 **语音的幅度谱 (magnitude spectrum)**；模型输出逐帧得分，再通过平均池化得到话语级（utterance-level）得分。                                                                                                                                                                                                                                |
| **模型结构**         | - **LDNet**：统一的 Listener-Dependent 模型，输入包括语音特征和听音者 ID，直接预测 LD 分数。<br>- **编码器-解码器结构**：编码器学习 Listener-Independent 特征，解码器结合听音者信息生成 Listener-Dependent 特征。<br>- 尝试了多种编码器：MBNet-style Conv2d、MobileNetV2、MobileNetV3；解码器采用 FFN 或 RNN。<br>- 扩展版本：**LDNet-MN**（多任务学习，利用均值分数辅助训练），**LDNet-ML**（引入虚拟的“平均听音者”，提高推理效率）。 |
| **模型输出 MOS 的误差** | - **评估指标**：MSE（误差）、LCC（Pearson 线性相关）、SRCC（Spearman 秩相关）。<br>- **VCC2018 数据集**：最佳模型（LDNet MobileV3/FFN）系统级 MSE = **0.013**，LCC = **0.988**，SRCC = **0.976**。<br>- **BVCC 数据集**：最佳模型系统级 MSE = **0.157**，LCC = **0.881**，SRCC = **0.881**。<br>- 相比 MOSNet、MBNet，LDNet 系统级误差显著降低，相关性更高。                          |
| **引用数**          | 论文是 2021 年提出并在后续改进，最新版本（你上传的 PDF）应为 **2022–2023 前后 arXiv/会议论文**。目前在 **Google Scholar 上大约 20–40 次引用**（属于近年活跃方向）。                                                                                                                                                                                              |


---

### EMDSQA: Neural Speech Quality Assessment with Speaker Embedding——2024
数据集	
- 训练集：NISQA 语料库，过滤后得到 **9537 样本（2216名说话人）**用于训练，**2237 样本（426名说话人）**用于验证，采样率 48 kHz。
- 测试集：包含两个部分：① NISQA 英语测试集（120 个样本）；② 作者自建的 Mandarin + Online Meeting 测试集（含匹配参考 MR 和不匹配参考 NMR 两种任务，每类 120 样本，涵盖噪声、丢包、非线性处理、低音量等失真类型）。

是否涉及编码器损伤	提及的失真类型主要包括 背景噪声、丢包、非线性处理、低音量，但未明确涉及“编解码压缩损伤”。重点在于 真实通信场景的退化音频。

标签工具与方法	主观 MOS 标签：遵循 ITU-T P.808 进行众包式测试；在部分测试中使用 MUSHRA (ITU-R BS.1534) 标准以降低主观偏差。

特征输入	
- 从失真语音提取 Mel-spectrogram 特征；
- 同时引入 说话人嵌入（Speaker Embedding）（由 U-Net 自注意力模型训练提取，或 ECAPA-TDNN 对照）作为先验信息输入 MOS 预测网络。

模型结构	
- 基于 自注意力 (Self-Attention) 的 MOS 预测网络；
- 说话人嵌入通过独立管道提取，并与 MOS 模型隐藏层向量拼接；
- 引入 Cosine Similarity Loss + MOS 损失 联合训练，以优化嵌入与 MOS 的一致性；
- 低复杂度，推理仅需 CPU 1.5% 实时因子。

模型输出及误差	
输出为预测的 MOS 分数 (1–5)。
- MR (匹配参考) 任务：PCC = 0.92，RMSE = 0.45；
- NMR (不匹配参考) 任务：依旧保持领先表现，显著优于对比方法（如 DNSMOS、NISQA、POLQA 等）。

引用数	论文发表于 IEEE Signal Processing Letters (2024, Vol.31)，DOI: 10.1109/LSP.2024.3478211。目前为最新论文，预计引用数接近 0–若干。

1. 研究背景

问题背景：在线语音通信（如 Zoom、Skype、DingTalk、Tencent Meeting 等）大量普及，但通信质量监测手段不足。传统 MOS（Mean Opinion Score）主观测试 虽然权威（ITU-T P.800/P.808），但耗时耗力，无法实时应用。

传统方法问题：

PESQ、POLQA：需要干净参考音频（侵入式 intrusive），在线通信场景通常无法获取参考。

非侵入式模型（DNSMOS、NISQA 等）：不需要参考，但在真实复杂场景下的鲁棒性和泛化能力不足。

创新点：

作者提出引入 Speaker Embedding（说话人嵌入），作为辅助先验特征输入 MOS 预测模型。

说话人特征可提高 MOS 预测的个性化与鲁棒性，尤其适用于跨说话人、跨场景的真实通信。

2. 数据集

训练集：

使用 NISQA 语料库，去掉参考音质差的样本。

最终得到 9537 条训练样本（2216 说话人），2237 条验证样本（426 说话人）。

采样率 48kHz，均为英语。

MOS 标签按照 ITU-T P.808 众包测试方式获得。

测试集：

英语测试集（NISQA corpus，120 样本）。

自建汉语测试集（Mandarin corpus）：

匹配参考任务（MRs）：120 条样本。

非匹配参考任务（NMRs）：120 条样本。

覆盖失真类型：背景噪声、丢包、非线性处理、低音量，每种占比 20%。

MOS 标签：结合 ITU-T P.808 与 MUSHRA (BS.1534) 测试获得。

3. 输入与输出

输入：

Mel-spectrogram 特征：由失真语音提取。

说话人嵌入：通过独立训练的 U-Net 自注意力模型提取，或者对比用 ECAPA-TDNN 提取。

输出：

预测 MOS 分数（范围 1–5）。

输出既能处理 MRs（匹配参考） 场景，也能处理 NMRs（非匹配参考） 场景。

4. 模型与方法细节

核心结构：

说话人嵌入侧

用专门训练的 U-Net 自注意力提取器（Stage-1 得到）对参考/注册语音提取说话人嵌入；再经 下采样块2 编码成“任务特化”的嵌入向量。

失真语音侧

失真语音 → 特征提取/下采样块1 → 隐藏向量。

融合与注意力

将“编码后的说话人嵌入”与“失真语音隐藏向量”拼接；分别送入 两个 attention-pooling 分支，得到两个 64 维表征 

损失函数（训练关键）

MOS 主损失：与 (a)(b) 一样采用 bias-aware MSE 面向 MOS 的回归误差。

余弦相似度损失 施加 cosine similarity 约束，使说话人嵌入与失真语音表征在任务空间对齐；训练时与 MOS 损失联合优化。推理阶段只需事先登记(enroll) 每位说话人的嵌入即可用于会议全程质量评估。

训练流程：

Stage 1：独立训练 speaker embedding 提取器（U-Net 自注意力）。

Stage 2：冻结 embedding 模块后，训练 MOS 预测模型（带 Cosine Similarity Loss + MSE Loss）。

5. 仿真与结果

对比方法：

传统：PESQ、POLQA。

非侵入式：DNSMOS、NISQA。

Speaker embedding 方法：ivec-MOS、i-MBFormer、NORESQA-MOS。

结果指标：

PCC（Pearson Correlation Coefficient）：预测结果与真实 MOS 的相关性，越大越好。

RMSE（Root Mean Square Error）：预测结果与真实 MOS 的平均误差，越小越好。

实验结果：

MRs 任务（匹配参考）：

EMDSQA 达到 PCC = 0.92，RMSE = 0.45。

显著优于 POLQA（侵入式）和 NISQA、DNSMOS（非侵入式）。

NMRs 任务（不匹配参考）：

EMDSQA 仍保持领先，说明其在跨语种（汉语/英语）、跨参考场景下鲁棒性更强。

效率：

在 CPU 上运行时推理仅需 1.5% 实时因子，可用于实时在线通信质量监测。

6. 误差分析

误差指标：

使用 RMSE（Root Mean Square Error） 衡量预测 MOS 与真实 MOS 的偏差。

在 MRs 任务中，EMDSQA 的 RMSE = 0.45；

在 NMRs 任务中，RMSE 略有上升，但仍显著低于对比方法。

结论：

相比传统 PESQ/POLQA，EMDSQA 无需参考音频，更适合在线通信。

相比 DNSMOS、NISQA 等非侵入式方法，EMDSQA 在 误差更低、相关性更高，说明 speaker embedding 确实提升了模型性能和泛化能力。


---
### Non-Intrusive Audio Quality Assessment Based on DNN for MOS Prediction——2024
数据集	使用 Free Music Archive (FMA) 数据集，具体为 fma-small 子集，包含 8,000 样本，44.1kHz 采样率，每段 30s。作者将其切分为 10s 片段，并在其上加入失真（噪声、静态、混响、压缩、丢包、滤波等）。训练/验证/微调/测试划分比例为 25:5:1:1。

是否涉及编码器损伤	是的，论文在模拟失真时引入了 编解码压缩损伤，使用 Opus 和 AVS2-P3 编码器，不同码率下生成压缩失真样本。

标签工具与方法	两阶段：
1. 预训练阶段：使用客观指标作为标签，包括 PEAQ、VISQOL、Si-SNR、WSS、LLR。
2. 微调阶段：使用 主观 MOS 标签（由 36 名听音者评分，每段 ≥4 个评分，取平均值）。

特征输入	Mel-spectrogram + Constant-Q Transform (CQT) 作为双通道输入（每个帧长 1024，hop size 512，112 维特征），拼接后送入网络。

模型结构	
- 六层 Gated Convolution (GConv2d)，多尺度卷积提取特征
- 双向 GRU (BiGRU) ×4：前两层做时序建模，后两层双路并行处理“score/distance”指标
- FC-Attention 层：区分指标类别
- 全连接层 (Dense blocks) 输出 MOS。
  
模型输出及误差	输出为预测的 MOS 分数 (1–5)。\
最终结果（带所有模块）：MSE = 0.230, RMSE = 0.480, PLCC = 0.920, SROCC = 0.892。对比传统方法（PEAQ、POLQA、VISQOL），误差更低、相关性更高。

引用数	论文发表于 2024 ISCSLP (International Symposium on Chinese Spoken Language Processing)。目前是 IEEE Xplore 收录论文，由于较新（2024 年），引用数暂时可能接近 0–数个。
