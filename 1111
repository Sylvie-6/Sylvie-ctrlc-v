1. 研究背景与动机

VoIP 音频质量下降的来源：环境噪声、网络劣化（丢包、时延、抖动）、编解码算法。

问题：传统的客观方法（如 PESQ、POLQA、P.563）要么需要参考信号（侵入式），要么在噪声/网络失真下与主观 MOS 的相关性很差。

目标：提出一个 轻量级、非侵入式 DNN 模型，能在嵌入式系统（如低端 VoIP 终端）上实时运行，同时能在噪声和网络失真条件下预测语音质量。

2. 数据集设计

论文用了 三类数据集：

公开噪声语音数据库（用于语音增强/TTS）——只含环境噪声，没有网络劣化；标签来自 PESQ。

NISQA Corpus——约 14,000 条语音，涵盖 Skype、Zoom、WhatsApp 等应用的真实采集，带 MOS 标签（人工评分）。

人工生成的 Noise–Network 数据集：

干净语音 + 环境噪声（8 种，SNR=0,5,10,15 dB）

再通过仿真网络加入：丢包 (0–35%)、时延 (0–500 ms)、抖动 (0–40%)，编码器 G.722

结果：带有噪声参数和网络参数的“可控退化”数据，可用来分析模型在不同条件下的表现。

3. MiniatureVQNet 模型架构

模型被设计成非常小（仅 5689 个可训练参数），可以部署在嵌入式环境：

特征提取：基于改进版 DeepSpeech 前端，不依赖手工特征（MFCC 等），直接从语音频谱里学习噪声、失真特征。

结构：

4 层全连接层 (32-32-32-8, ReLU)

2 层双向 GRU (每层 8 单元)

2 层全连接层 (8-1) → 输出预测分数

优化：SGD + 学习率指数衰减 (初始 0.01, decay rate=0.9, decay steps=1000)，batch=64。

4. 模型优化（嵌入式部署）

量化 (Quantization)：降低模型权重精度，减少内存占用和运算量。

Float16 量化：精度基本无损（相关性≈0.690, MSE≈0.194）。

全整数量化 (int8)：MSE 略升到 0.254，但仍可用。

结论：可把模型从 float32 压缩到 float16，在不损失精度的情况下显著降低资源占用。

5. 实验与结果

基线对比：ITU-T P.563

MSE：P.563 = 2.19；MiniatureVQNet-Noise = 0.34；MiniatureVQNet-Noise–Network = 0.21
→ 新方法在所有测试条件下都显著优于 P.563。

噪声+网络条件下的性能：

模型在低 SNR（0–10 dB）时性能下降，但 Noise–Network 版本始终优于 Noise-only。

不同噪声类型（如街道 vs. 车站）：复杂噪声下（车站）性能下降更明显。

抖动 (jitter)：200 ms 时延下，10% 抖动差异最大；Noise–Network 版本显著优于 Noise-only。

丢包 (packet loss)：随着丢包率增加，两种模型精度下降，但 Noise–Network 始终更稳健。


1. 研究背景与目标

背景：在 LTE-A 移动网络环境下，真实 VoIP 语音流（40,000 个 RTP 包）被采集。

目标：对 6 个关键 QoS/QoE 指标（多元时间序列）进行建模与预测，以便网络运营商提前进行资源调度和 QoE 管理。

指标：

MOS（Mean Opinion Score）

BW（带宽消耗）

RTT（往返时延）

Jitter（抖动）

DJB（去抖动缓冲区）

SNR（信噪比）

2. 数据收集与处理

实验平台：

车载终端 UE1（Linphone 软电话，支持 RTCP-XR 协议） ↔ UE2（固定 PC + Linphone + Wireshark）。

驾驶 70 公里路线，采集到 约 40,000 RTP 语音包。

使用 Opus 编码（6–128 kbps，可变码率，48kHz 采样）。

指标提取：通过 Wireshark + RTCP-XR 报文字段提取：

MOS ← 从 R-Factor 转换 (ITU G.107)

BW ← RTP 流直接统计 kb/s

RTT ← rtcp.xr.voipmetrics.rtdelay

JIT ← frame.time delta 计算分组间隔差

DJB ← rtcp.xr.voipmetrics.jbmax

SNR ← (signal level − noise level)

3. 方法流程

预处理：VAR (Vector Autoregressive) 模型

将六个指标建模为多元时间序列，捕捉相互依赖。

使用 Akaike 信息准则 (AIC) 估计最佳滞后阶数 p=11。

检查平稳性（特征值 <1，确保稳定）和残差独立性（Breusch–Godfrey 检验，p=0.4287 → 不拒绝 H0，残差无自相关）。

转化为监督学习：滑动窗口

输入：过去 p=11 步的多指标向量。

输出：下一时刻的指标值（单步预测为主）。

神经网络预测

模型对比：

RNN：基础结构，易受梯度消失影响。

LSTM：引入记忆单元与门控，解决长期依赖。

GRU：简化的 LSTM（更新门 + 重置门）。

配置：单层，50 单元；SGD 优化器，lr=0.01，训练 30 epoch；特征归一化到 [0,1]。

4. 实验结果

预测效果（见 Fig.5）：

MOS 在 4.38–4.5 范围，预测误差很小；

RTT、Jitter 波动大，RNN 捕捉不稳，LSTM/GRU 更好；

DJB 与 Jitter 同步震荡，GRU 能较好跟踪；

SNR 有明显波动，但仍能预测趋势。

定量结果（见 Table I）：

MOS：GRU 最优（MAE=0.0056, RMSE=0.0059）；

RTT/Jitter/DJB：LSTM 和 GRU 优于 RNN；

整体：GRU 与 LSTM 交替最佳，GRU 在性能–计算效率折中上更优。

计算复杂度（见 Fig.6）：

RNN 时间复杂度最低；

LSTM 随单元数增加，计算时间显著上升；

GRU 增幅较温和，被认为是最佳折中选择。



1. 论文做了什么（一眼看懂）

任务：无参考（non-intrusive）下，对失真音频直接回归 MOS (1–5)。输入只要失真音频，不要干净参考。

特征：把 Mel 频谱与 CQT 拼成双通道输入（高采样率音频里，CQT能补充对音乐音高/谐波的刻画，Mel更贴近听觉感知）。帧长 1024、hop 512、频带维度 112。

网络：前端 6 个 GConv2d 门控卷积块抽特征 → 2 层 BiGRU整合时序 → 双路径 BiGRU分别对“分数类指标（score）”与“距离类指标（distance）”做子任务学习 → Dense 头；再在双路径后加一个 FC-attention 轻量注意力。

两步训练：

Step-1 预训练：用客观指标做标签（score: PEAQ、VISQOL；distance: SI-SNR、WSS、LLR），并给 distance 指标配上 EdgeLoss；

Step-2 微调：再用主观 MOS做标签，端到端对齐人耳感知。

结果：全模块打开时 MSE=0.230，RMSE=0.480，PLCC=0.920，SROCC=0.892，显著优于 PEAQ/POLQA/VISQOL（分数映射到 1–5 后对比）。消融显示：GConv2d、FC-attention、两步训练、混合特征（Mel+CQT）都带来增益。

2. 数据与失真模拟（含“丢包/掉帧”）

数据：FMA-small（8,000 段、8 个流派、44.1 kHz，每段 30 s），先过滤低质样本，再切 10 s 片段并划分：预训练/微调/验证/测试 ≈ 25/5/1/1。微调/验证/测试由受试者给出 MOS（每段≥4 人，取均值）。

六类退化：噪声（多 SNR）、静态失真（帧级增益跳变）、混响（pyroomacoustics，变房间/RT60）、编码压缩（Opus、AVS2-P3，多码率）、dropout 掉帧/丢包（Gilbert-Elliot 模型生成帧级 mask，模拟突发与随机丢包）、滤波（高/低通不同截止频）。这一点很贴近你 VoIP 的关心点：论文里明确模拟了丢包，但丢包不是作为网络输入特征，而是用于生成退化音频。

3. 特征与输入张量

双通道输入：

通道1：Mel-spectrogram（112 维带）

通道2：CQT（112 维带）
两者按时间对齐后在“通道维”拼接，进入 2D 卷积前端；设置帧长 1024、hop 512以兼顾高采样率音频的时频分辨率。

4. 网络结构（逐层拆解）

(1) 前端：6× GConv2d（门控卷积）

每个 GConv2d 块 = 主卷积支路（Conv2d+BN+Dropout） × 门控支路（Conv2d+Sigmoid），输出是两支路的逐点乘积（让网络自动“抑制不重要区域，强调关键信号”）。卷积核尺寸逐层变化（多尺度感受野）。示意如图 2。

通道与核/步长（摘自 Table 1）：

2→16, k=(3×3), stride (1,2)

16→32, k=(3×5), s (1,2)

32→64, k=(3×5), s (1,3)

64→128, k=(3×5), s (1,1)

128→256, k=(3×3), s (1,1)

256→512, k=(3×1), s (1,1)
通过在“时间维”逐步下采样（如 stride 的 2/3），压缩序列长度，保留时频关键纹理。

(2) 时序整合：2× BiGRU

在卷积后接两层 BiGRU，融合前后文依赖（音频质量的感知有显著的时间上下文）。

(3) 双路径子任务：score vs. distance

再接两条平行 BiGRU：

score 支路回归PEAQ、VISQOL（这类“分数型”客观指标）

distance 支路回归 SI-SNR、WSS、LLR（“距离/相似度度量”类）

两支路后各接 Dense 头输出对应指标；在双路径之后再加一个轻量 FC-attention（文中叫 PC-attention），让网络区分两类指标的“关注点”并加权。

(4) MOS 头

预训练完成后，切到第二阶段，用主观 MOS做标签进行微调，最终输出单一的 MOS 预测（1–5 缩放域）。

5. 训练目标与 EdgeLoss

Step-1（预训练）：

score 指标（PEAQ、VISQOL）→ MSE；

distance 指标（SI-SNR、WSS、LLR）→ EdgeLoss：

直觉：这些度量的“数值间隔”不等价（越到极端区间，单位间隔代表的“感知差别”反而变小/对训练有害）。

做法：用带阈值 T与**权重 β(x,y)**的加权 L1/变体，超阈后惩罚增长速率降低，避免极端样本主导梯度。

总损失 = MSE(PEAQ)+MSE(VISQOL)+λ₁·Edge(SI-SNR)+λ₂·Edge(WSS)+λ₃·Edge(LLR)。文中示例：λ₁=0.001，λ₂=0.1，λ₃=0.001；WSS/LLR 的阈值 50/10，SI-SNR 用 ±30。

Step-2（微调）：MOS→MSE。

6. 训练细节（复现实用）

优化器/超参：Adam，lr=1e-4；Dropout=0.2；

批量/轮次：预训练 batch=128，最少 300 个 epoch，早停 patience=10；微调 batch=32，最少 100 个 epoch，早停 patience=10；使用 RTX 3090 训练。
